{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378a0076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26df6e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "input_size = 128*128\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8117124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'vegetables', 'subcategory': 'leafy', 'ingredient': 'romaine', 'image': <PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=533x849 at 0x1D7D4573570>}\n"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "ds = load_dataset(\"Scuccorese/food-ingredients-dataset\", split=\"train[:2000]\") #2000 images pulled \n",
    "ingredients = sorted(set(ds[\"ingredient\"]))\n",
    "ds = ds.train_test_split(test_size=0.2) #split data into training data and validation data\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b85b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 100\n",
      "adzuki beans\n"
     ]
    }
   ],
   "source": [
    "#create dictionary that maps label name to integer\n",
    "label2id = {name: str(i) for i, name in enumerate(ingredients)}\n",
    "id2label = {str(i): name for i, name in enumerate(ingredients)}\n",
    "\n",
    "num_classes = len(ingredients)\n",
    "print(\"Number of Classes:\", num_classes)\n",
    "\n",
    "print(id2label[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95922591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1600/1600 [01:43<00:00, 15.52 examples/s]\n",
      "Map: 100%|██████████| 400/400 [00:35<00:00, 11.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor([0.6136, 0.5784, 0.4710]) standard deviation:  tensor([0.2960, 0.2859, 0.3242])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1600/1600 [00:13<00:00, 116.40 examples/s]\n",
      "Map: 100%|██████████| 400/400 [00:03<00:00, 104.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#process image into tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),          # Resize images to 128x128 (this number can be changed for different resolutions)\n",
    "    transforms.ToTensor(),                 # Convert PIL image to tensor\n",
    "])\n",
    "\n",
    "def transform_fn(data):\n",
    "    image = data[\"image\"].convert(\"RGB\") #make all images RGB to avoid initial channel mismatches\n",
    "    data[\"image\"] = transform(image)\n",
    "    data[\"label\"] = int(label2id[data[\"ingredient\"]])\n",
    "    return data\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(transform_fn)\n",
    "ds[\"test\"]  = ds[\"test\"].map(transform_fn)\n",
    "\n",
    "#convert to pytorch tensor\n",
    "ds[\"train\"] = ds[\"train\"].with_format(\"torch\", columns=[\"image\", \"label\"])\n",
    "ds[\"test\"] = ds[\"test\"].with_format(\"torch\", columns=[\"image\", \"label\"])\n",
    "\n",
    "#compute mean and std for normalization\n",
    "all_images = torch.stack([img for img in ds[\"train\"][\"image\"]], dim=0)\n",
    "mean, std = all_images.mean(dim=[0, 2, 3]), all_images.std(dim=[0, 2, 3])\n",
    "print(\"mean: \", mean, \"standard deviation: \", std)\n",
    "\n",
    "#normalize dataset\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "def transform_norm_fn(data):\n",
    "    data[\"image\"] = transform_norm(data[\"image\"])\n",
    "    return data\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(transform_norm_fn)\n",
    "ds[\"test\"] = ds[\"test\"].map(transform_norm_fn)\n",
    "\n",
    "#create dataloaders\n",
    "train_loader = DataLoader(dataset=ds[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=ds[\"test\"], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b6a9a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb85c05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Loss: 4.3886 | Accuracy: 8.69%\n",
      "Epoch [2/10]\n",
      "Loss: 2.5950 | Accuracy: 60.38%\n",
      "Epoch [3/10]\n",
      "Loss: 1.6796 | Accuracy: 83.56%\n",
      "Epoch [4/10]\n",
      "Loss: 1.0589 | Accuracy: 90.44%\n",
      "Epoch [5/10]\n",
      "Loss: 0.6851 | Accuracy: 93.00%\n",
      "Epoch [6/10]\n",
      "Loss: 0.4478 | Accuracy: 94.25%\n",
      "Epoch [7/10]\n",
      "Loss: 0.3143 | Accuracy: 94.75%\n",
      "Epoch [8/10]\n",
      "Loss: 0.2385 | Accuracy: 94.56%\n",
      "Epoch [9/10]\n",
      "Loss: 0.1996 | Accuracy: 95.06%\n",
      "Epoch [10/10]\n",
      "Loss: 0.1817 | Accuracy: 94.56%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_loss = float('inf')  # Start with very large loss so first epoch always improves\n",
    "epochs_without_improvement = 0  # Counter for consecutive non-improving epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()  # Set model to training mode (enables dropout, batchnorm updates)\n",
    "\n",
    "    running_loss = 0.0  # Accumulate loss over the epoch\n",
    "    correct = 0         # Count correct predictions\n",
    "    total = 0           # Count total samples processed\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # Move data to GPU (if available)\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass: compute predictions\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss between predictions and true labels\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Clear previous gradients (PyTorch accumulates gradients by default)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this batch's loss to running total\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get predicted class (index of max logit)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update total sample count\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Compute accuracy percentage\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    print(f'Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "\n",
    "    # If loss improved, reset counter\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        # If loss did not improve, increment counter\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # If model has not improved for 'patience' epochs, stop training\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e310894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save trained model\n",
    "PATH = './food_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30861aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 44.0 %\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy of the network on the validation dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b348a2-f022-4a58-af68-22b5ee08d6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
