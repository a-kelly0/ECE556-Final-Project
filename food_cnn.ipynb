{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378a0076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arb77\\Desktop\\ODDL_FINAL_Project\\ECE556-Final-Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26df6e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "input_size = 128*128\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8117124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'legumes', 'subcategory': 'beans', 'ingredient': 'pinto beans', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x559 at 0x1D8238CF4D0>}\n"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "ds = load_dataset(\"Scuccorese/food-ingredients-dataset\", split=\"train[:2000]\") #2000 images pulled \n",
    "ingredients = sorted(set(ds[\"ingredient\"]))\n",
    "ds = ds.train_test_split(test_size=0.2) #split data into training data and validation data\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b85b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 100\n",
      "adzuki beans\n"
     ]
    }
   ],
   "source": [
    "#create dictionary that maps label name to integer\n",
    "label2id = {name: str(i) for i, name in enumerate(ingredients)}\n",
    "id2label = {str(i): name for i, name in enumerate(ingredients)}\n",
    "\n",
    "num_classes = len(ingredients)\n",
    "print(\"Number of Classes:\", num_classes)\n",
    "\n",
    "print(id2label[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95922591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1600/1600 [01:40<00:00, 15.90 examples/s]\n",
      "Map: 100%|██████████| 400/400 [00:33<00:00, 11.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor([0.6157, 0.5781, 0.4708]) standard deviation:  tensor([0.2964, 0.2871, 0.3249])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1600/1600 [00:15<00:00, 106.62 examples/s]\n",
      "Map: 100%|██████████| 400/400 [00:03<00:00, 103.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#process image into tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),          # Resize images to 128x128 (this number can be changed for different resolutions)\n",
    "    transforms.ToTensor(),                 # Convert PIL image to tensor\n",
    "])\n",
    "\n",
    "def transform_fn(data):\n",
    "    image = data[\"image\"].convert(\"RGB\") #make all images RGB to avoid initial channel mismatches\n",
    "    data[\"image\"] = transform(image)\n",
    "    data[\"label\"] = int(label2id[data[\"ingredient\"]])\n",
    "    return data\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(transform_fn)\n",
    "ds[\"test\"]  = ds[\"test\"].map(transform_fn)\n",
    "\n",
    "#convert to pytorch tensor\n",
    "ds[\"train\"] = ds[\"train\"].with_format(\"torch\", columns=[\"image\", \"label\"])\n",
    "ds[\"test\"] = ds[\"test\"].with_format(\"torch\", columns=[\"image\", \"label\"])\n",
    "\n",
    "#compute mean and std for normalization\n",
    "all_images = torch.stack([img for img in ds[\"train\"][\"image\"]], dim=0)\n",
    "mean, std = all_images.mean(dim=[0, 2, 3]), all_images.std(dim=[0, 2, 3])\n",
    "print(\"mean: \", mean, \"standard deviation: \", std)\n",
    "\n",
    "#normalize dataset\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "def transform_norm_fn(data):\n",
    "    data[\"image\"] = transform_norm(data[\"image\"])\n",
    "    return data\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(transform_norm_fn)\n",
    "ds[\"test\"] = ds[\"test\"].map(transform_norm_fn)\n",
    "\n",
    "#create dataloaders\n",
    "train_loader = DataLoader(dataset=ds[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=ds[\"test\"], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6a9a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb85c05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     20\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m     24\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arb77\\Desktop\\ODDL_FINAL_Project\\ECE556-Final-Project\\.venv\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arb77\\Desktop\\ODDL_FINAL_Project\\ECE556-Final-Project\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arb77\\Desktop\\ODDL_FINAL_Project\\ECE556-Final-Project\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_loss = float('inf')  # Start with very large loss so first epoch always improves\n",
    "epochs_without_improvement = 0  # Counter for consecutive non-improving epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()  # Set model to training mode (enables dropout, batchnorm updates)\n",
    "\n",
    "    running_loss = 0.0  # Accumulate loss over the epoch\n",
    "    correct = 0         # Count correct predictions\n",
    "    total = 0           # Count total samples processed\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # Move data to GPU (if available)\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass: compute predictions\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss between predictions and true labels\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Clear previous gradients (PyTorch accumulates gradients by default)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this batch's loss to running total\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get predicted class (index of max logit)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update total sample count\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Compute accuracy percentage\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    print(f'Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "\n",
    "    # If loss improved, reset counter\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        # If loss did not improve, increment counter\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # If model has not improved for 'patience' epochs, stop training\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e310894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save trained model\n",
    "PATH = './food_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30861aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 43.25 %\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy of the network on the validation dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b348a2-f022-4a58-af68-22b5ee08d6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
