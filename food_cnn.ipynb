{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378a0076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alkelly2/ece556/myvenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26df6e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "input_size = 128*128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8117124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'vegetables', 'subcategory': 'stem', 'ingredient': 'rhubarb', 'image': <PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=1067x1690 at 0x7FFE470EE900>}\n"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "ds = load_dataset(\"Scuccorese/food-ingredients-dataset\", split=\"train[:2000]\")\n",
    "ingredients = sorted(set(ds[\"ingredient\"]))\n",
    "ds = ds.train_test_split(test_size=0.2) #split data into training data and validation data\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b85b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 100\n",
      "adzuki beans\n"
     ]
    }
   ],
   "source": [
    "#create dictionary that maps label name to integer\n",
    "label2id = {name: str(i) for i, name in enumerate(ingredients)}\n",
    "id2label = {str(i): name for i, name in enumerate(ingredients)}\n",
    "\n",
    "num_classes = len(ingredients)\n",
    "print(\"Number of Classes:\", num_classes)\n",
    "\n",
    "print(id2label[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95922591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1600/1600 [01:28<00:00, 18.12 examples/s]\n",
      "Map: 100%|██████████| 400/400 [00:28<00:00, 14.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor([0.6131, 0.5764, 0.4677]) standard deviation:  tensor([0.2957, 0.2862, 0.3245])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1600/1600 [00:12<00:00, 123.79 examples/s]\n",
      "Map: 100%|██████████| 400/400 [00:03<00:00, 122.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#process image into tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),          # Resize images to 128x128 (this number can be changed for different resolutions)\n",
    "    transforms.ToTensor(),                 # Convert PIL image to tensor\n",
    "])\n",
    "\n",
    "def transform_fn(data):\n",
    "    image = data[\"image\"].convert(\"RGB\") #make all images RGB to avoid initial channel mismatches\n",
    "    data[\"image\"] = transform(image)\n",
    "    data[\"label\"] = int(label2id[data[\"ingredient\"]])\n",
    "    return data\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(transform_fn)\n",
    "ds[\"test\"]  = ds[\"test\"].map(transform_fn)\n",
    "\n",
    "#convert to pytorch tensor\n",
    "ds[\"train\"] = ds[\"train\"].with_format(\"torch\", columns=[\"image\", \"label\"])\n",
    "ds[\"test\"] = ds[\"test\"].with_format(\"torch\", columns=[\"image\", \"label\"])\n",
    "\n",
    "#compute mean and std for normalization\n",
    "all_images = torch.stack([img for img in ds[\"train\"][\"image\"]], dim=0)\n",
    "mean, std = all_images.mean(dim=[0, 2, 3]), all_images.std(dim=[0, 2, 3])\n",
    "print(\"mean: \", mean, \"standard deviation: \", std)\n",
    "\n",
    "#normalize dataset\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "def transform_norm_fn(data):\n",
    "    data[\"image\"] = transform_norm(data[\"image\"])\n",
    "    return data\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(transform_norm_fn)\n",
    "ds[\"test\"] = ds[\"test\"].map(transform_norm_fn)\n",
    "\n",
    "#create dataloaders\n",
    "train_loader = DataLoader(dataset=ds[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=ds[\"test\"], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b6a9a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Simple CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),   \n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),   \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*32*32, 128),  # 64 channels, 32x32 feature map\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb85c05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "[1/10], loss: 7.756 accuracy: 1.438\n",
      "Epoch [2/10]\n",
      "[2/10], loss: 4.594 accuracy: 1.250\n",
      "Epoch [3/10]\n",
      "[3/10], loss: 4.591 accuracy: 1.625\n",
      "Epoch [4/10]\n",
      "[4/10], loss: 4.563 accuracy: 2.875\n",
      "Epoch [5/10]\n",
      "[5/10], loss: 4.528 accuracy: 3.188\n",
      "Epoch [6/10]\n",
      "[6/10], loss: 4.506 accuracy: 3.188\n",
      "Epoch [7/10]\n",
      "[7/10], loss: 4.477 accuracy: 3.500\n",
      "Epoch [8/10]\n",
      "[8/10], loss: 4.461 accuracy: 3.312\n",
      "Epoch [9/10]\n",
      "[9/10], loss: 4.430 accuracy: 3.625\n",
      "Epoch [10/10]\n",
      "[10/10], loss: 4.408 accuracy: 3.375\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=num_classes)\n",
    "\n",
    "#Send model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Train the model\n",
    "#Loop through dataset and update the model's weights\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() #switch to train mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\") #display current epoch\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #measure running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #measure running accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    #print statistics\n",
    "    epoch_loss = running_loss / len(train_loader) #loss scaled for size of dataset\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    print(f'[{epoch+1}/{num_epochs}], loss: {epoch_loss:.3f} accuracy: {epoch_acc:.3f}')\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e310894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save trained model\n",
    "PATH = './food_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30861aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 2.75 %\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy of the network on the validation dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b348a2-f022-4a58-af68-22b5ee08d6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
