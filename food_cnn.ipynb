{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378a0076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pu441\\Documents\\ECE556\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26df6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "input_size = 128*128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8117124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'vegetables', 'subcategory': 'root', 'ingredient': 'radish', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1200x1600 at 0x1DE8A9FDD90>}\n"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "ds = load_dataset(\"Scuccorese/food-ingredients-dataset\", split=\"train[:500]\")\n",
    "ingredients = sorted(set(ds[\"ingredient\"]))\n",
    "ds = ds.train_test_split(test_size=0.2) #split data into training data and validation data\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b85b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 26\n",
      "arugula\n"
     ]
    }
   ],
   "source": [
    "#create dictionary that maps label name to integer\n",
    "label2id = {name: str(i) for i, name in enumerate(ingredients)}\n",
    "id2label = {str(i): name for i, name in enumerate(ingredients)}\n",
    "\n",
    "num_classes = len(ingredients)\n",
    "print(\"Number of Classes:\", num_classes)\n",
    "\n",
    "print(id2label[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95922591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:26<00:00, 14.88 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:06<00:00, 15.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#process image into tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),          # Resize images to 128x128 (this number can be changed for different resolutions)\n",
    "    transforms.ToTensor(),                 # Convert PIL image to tensor\n",
    "    transforms.Normalize([0.5,0.5,0.5],    # Normalize\n",
    "                         [0.5,0.5,0.5]) #TODO normalize to mean of dataset\n",
    "])\n",
    "\n",
    "def transform_fn(data):\n",
    "    image = data[\"image\"].convert(\"RGB\") #make all images RGB to avoid initial channel mismatches\n",
    "    data[\"image\"] = transform(image)\n",
    "    data[\"label\"] = int(label2id[data[\"ingredient\"]])\n",
    "    return data\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(transform_fn)\n",
    "ds[\"test\"]  = ds[\"test\"].map(transform_fn)\n",
    "\n",
    "#convert to pytorch tensor\n",
    "ds[\"train\"] = ds[\"train\"].with_format(\"torch\", columns=[\"image\", \"label\"])\n",
    "ds[\"test\"] = ds[\"test\"].with_format(\"torch\", columns=[\"image\", \"label\"])\n",
    "\n",
    "#create dataloaders\n",
    "train_loader = DataLoader(dataset=ds[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=ds[\"test\"], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b6a9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple cnn\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),   # 128 -> 64\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)    # 64 -> 32\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*32*32, 128),  # 32 channels, 32x32 feature map\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85c05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "[1/10], loss: 3.338 accuracy: 9.000\n",
      "Epoch [2/10]\n",
      "[2/10], loss: 2.941 accuracy: 18.250\n",
      "Epoch [3/10]\n",
      "[3/10], loss: 2.411 accuracy: 32.250\n",
      "Epoch [4/10]\n",
      "[4/10], loss: 1.755 accuracy: 47.250\n",
      "Epoch [5/10]\n",
      "[5/10], loss: 1.147 accuracy: 64.750\n",
      "Epoch [6/10]\n",
      "[6/10], loss: 0.776 accuracy: 78.750\n",
      "Epoch [7/10]\n",
      "[7/10], loss: 0.550 accuracy: 84.750\n",
      "Epoch [8/10]\n",
      "[8/10], loss: 0.258 accuracy: 94.000\n",
      "Epoch [9/10]\n",
      "[9/10], loss: 0.131 accuracy: 97.500\n",
      "Epoch [10/10]\n",
      "[10/10], loss: 0.069 accuracy: 99.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=num_classes)\n",
    "\n",
    "#Send model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Train the model\n",
    "#Loop through dataset and update the model's weights\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() #switch to train mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\") #display current epoch\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #measure running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #measure running accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    #print statistics\n",
    "    epoch_loss = running_loss / len(train_loader) #loss scaled for size of dataset\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    print(f'[{epoch+1}/{num_epochs}], loss: {epoch_loss:.3f} accuracy: {epoch_acc:.3f}')\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e310894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save trained model\n",
    "PATH = './food_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30861aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 500 test images: 37.0 %\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy of the network on the validation dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images = data[\"image\"]\n",
    "        labels = data[\"label\"]\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 500 test images: {100 * correct / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
